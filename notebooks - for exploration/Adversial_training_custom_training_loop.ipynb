{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---imports---\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import scanpy as sc\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Custom Training Loop---\n",
    "# Define the autoencoder adversarial training function\n",
    "@tf.function #overriding the normal training function\n",
    "def train_autoencoder_adversarial(gene_expression, batch_labels, autoencoder, discriminator, optimizer):\n",
    "    \"\"\"\n",
    "    Train the autoencoder to fool the discriminator into classifying reconstructions as target classes.\n",
    "    \n",
    "    Args:\n",
    "        gene_expression: Input gene_expression data\n",
    "        batch_labels: One-hot encoded target batch labels to fool the discriminator\n",
    "        encoder: Pretrained encoder model\n",
    "        decoder: Pretrained decoder model\n",
    "        discriminator: Pretrained discriminator model (frozen during this training)\n",
    "        optimizer: Optimizer for the autoencoder\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # CAll autoencoder to get reconstructed gene expression\n",
    "        reconstructed_gene_expression = autoencoder(gene_expression)\n",
    "        \n",
    "        # Get discriminator output for reconstructed gene expression\n",
    "        disc_output = discriminator(reconstructed_gene_expression)\n",
    "        \n",
    "        # Adversarial loss - make discriminator classify reconstructions as target labels\n",
    "        adversarial_loss = tf.keras.losses.CategoricalCrossentropy()(batch_labels, disc_output)\n",
    "    \n",
    "    # Get gradients and update autoencoder weights only\n",
    "    gradients = tape.gradient(adversarial_loss, autoencoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, autoencoder.trainable_variables))\n",
    "    \n",
    "    return adversarial_loss\n",
    "\n",
    "# Create a custom training loop\n",
    "def adversarial_training(dataset, epochs, autoencoder, discriminator):\n",
    "    # Freeze the discriminator weights\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    # Optimizer for the autoencoder\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        adv_loss_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "        for batch in dataset:\n",
    "            gene_expression, batch_labels = batch\n",
    "            \n",
    "            # Target same class as original\n",
    "            target_labels = batch_labels\n",
    "            \n",
    "            # Train autoencoder\n",
    "            adv_loss = train_autoencoder_adversarial(\n",
    "                gene_expression, target_labels, autoencoder, discriminator, optimizer\n",
    "            )\n",
    "            \n",
    "            # Update adv loss\n",
    "            adv_loss_avg.update_state(adv_loss)\n",
    "\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(\n",
    "              f\"Adversarial Loss: {adv_loss_avg.result():.4f}, \" \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---load data and Neuronal Networks---\n",
    "#set base path to load data: goes back one directory and then into the data\n",
    "base_path = os.path.join('..', 'data')\n",
    "#Name of data set\n",
    "dataset_name = 'large_atac_gene_activity'\n",
    "# read dataset into an anndata object:  Category - Cells of the brain\n",
    "inPath = os.path.join(base_path, f\"{dataset_name}.h5ad\")\n",
    "adata = sc.read(inPath)\n",
    "\n",
    "#set base path to load data: goes back one directory and then into the data\n",
    "base_path = os.path.join('..', 'src', 'models', 'saved_models')\n",
    "#Name of autoencoder\n",
    "dataset_name = 'autoencoder_mselossfunction'\n",
    "# load autoencoder\n",
    "inPath = os.path.join(base_path, f\"{dataset_name}.keras\")\n",
    "autoencoder = tf.keras.models.load_model(inPath)\n",
    "\n",
    "#Name of discriminator\n",
    "dataset_name = 'discriminator_pretrained'\n",
    "# load discriminator\n",
    "inPath = os.path.join(base_path, f\"{dataset_name}.keras\")\n",
    "discriminator = tf.keras.models.load_model(inPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Prepare Data---\n",
    "#ADATA->NUMPY\n",
    "GENE_EXPRESSION = adata.X.toarray()\n",
    "\n",
    "#One-hot encoded Batches\n",
    "encoder = OneHotEncoder(sparse_output=False)  # `sparse=False` returns a dense array\n",
    "BATCH_LABELS = encoder.fit_transform(adata.obs[['batchname_all']])\n",
    "\n",
    "#Combine in a Tensorflow dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((GENE_EXPRESSION, BATCH_LABELS))\n",
    "#Create batches\n",
    "batch_size = 30\n",
    "train_dataset = train_dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Adversarial Loss: 0.8067, \n",
      "Epoch 2/10\n",
      "Adversarial Loss: 0.9264, \n",
      "Epoch 3/10\n",
      "Adversarial Loss: 0.9540, \n",
      "Epoch 4/10\n",
      "Adversarial Loss: 0.8953, \n",
      "Epoch 5/10\n",
      "Adversarial Loss: 0.8297, \n",
      "Epoch 6/10\n",
      "Adversarial Loss: 0.7527, \n",
      "Epoch 7/10\n",
      "Adversarial Loss: 0.6886, \n",
      "Epoch 8/10\n",
      "Adversarial Loss: 0.6872, \n",
      "Epoch 9/10\n",
      "Adversarial Loss: 0.6182, \n",
      "Epoch 10/10\n",
      "Adversarial Loss: 0.5905, \n"
     ]
    }
   ],
   "source": [
    "#---Training---\n",
    "# Run the training\n",
    "epochs = 10\n",
    "adversarial_training(train_dataset, epochs, autoencoder, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRECTED_GENE_EXPRESSION = autoencoder(GENE_EXPRESSION) #test new autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Gene expression Discrimination:\n",
      "2651/2651 - 3s - 965us/step - accuracy: 0.3211 - loss: 1.7133\n",
      "Accuracy Correccted gene expression Discrimination:\n",
      "2651/2651 - 2s - 822us/step - accuracy: 0.0961 - loss: 6.1056\n"
     ]
    }
   ],
   "source": [
    " #test the discriminator\n",
    "print(\"Accuracy Gene expression Discrimination:\")\n",
    "score = discriminator.evaluate(GENE_EXPRESSION, BATCH_LABELS, verbose=2)\n",
    "print(\"Accuracy Correccted gene expression Discrimination:\")\n",
    "score = discriminator.evaluate(CORRECTED_GENE_EXPRESSION, BATCH_LABELS, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
